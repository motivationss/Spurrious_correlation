python run_expt.py -s confounder -d MultiNLI -t gold_label_random -c sentence2_has_negation --batch_size 32 --root_dir ./ --n_epochs 5 --aug_col None --log_dir results/MultiNLI/MultiNLI_sample_exp/ERM_upweight_0_epochs_5_lr_2e-05_weight_decay_0.0/model_outputs --metadata_path results/MultiNLI/MultiNLI_sample_exp/metadata_aug.csv --lr 2e-05 --weight_decay 0.0 --up_weight 0 --metadata_csv_name metadata.csv  --model bert --use_bert_params 0 --loss_type erm

saved in results/MultiNLI/MultiNLI_sample_exp/ERM_upweight_0_epochs_5_lr_2e-05_weight_decay_0.0/job.sh


wandb: Currently logged in as: gaotang (use `wandb login --relogin` to force relogin)





 WARNING, Using bert without using BERT HYPER-PARAMS 





wandb: wandb version 0.12.20 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.10.8
wandb: Syncing run eager-planet-4
wandb: ‚≠êÔ∏è View project at https://wandb.ai/gaotang/spurious_MultiNLI
wandb: üöÄ View run at https://wandb.ai/gaotang/spurious_MultiNLI/runs/1fcbil74
wandb: Run data is saved locally in wandb/run-20220630_211740-1fcbil74
wandb: Run `wandb off` to turn off syncing.
Dataset: MultiNLI
Shift type: confounder
Wandb: True
Project name: spurious
Target name: gold_label_random
Confounder names: ['sentence2_has_negation']
Up weight: 0
Resume: False
Minority fraction: None
Imbalance ratio: None
Fraction: 1.0
Root dir: ./
Reweight groups: False
Augment data: False
Val fraction: 0.1
Loss type: erm
Alpha: 0.2
Generalization adjustment: 0.0
Automatic adjustment: False
Robust step size: 0.01
Joint dro alpha: 1
Use normalized loss: False
Btl: False
Hinge: False
Model: bert
Train from scratch: False
N epochs: 5
Batch size: 32
Lr: 2e-05
Scheduler: False
Weight decay: 0.0
Gamma: 0.1
Minimum variational weight: 0
Seed: 0
Show progress: False
Log dir: results/MultiNLI/MultiNLI_sample_exp/ERM_upweight_0_epochs_5_lr_2e-05_weight_decay_0.0/model_outputs
Log every: 50
Save step: 10
Save best: False
Save last: False
Use bert params: 0
Num folds per sweep: 5
Num sweeps: 4
Q: 0.7
Metadata csv name: metadata.csv
Fold: None
Metadata path: results/MultiNLI/MultiNLI_sample_exp/metadata_aug.csv
Aug col: None

length of train_data:  206175
length of test_data:  123712
length of val_data:  82462
args fold:  None


WARNING: aug_col is not being used.


Training Data...
    gold_label_random = 0, sentence2_has_negation = 0: n = 57498
    gold_label_random = 0, sentence2_has_negation = 1: n = 11158
    gold_label_random = 1, sentence2_has_negation = 0: n = 67376
    gold_label_random = 1, sentence2_has_negation = 1: n = 1521
    gold_label_random = 2, sentence2_has_negation = 0: n = 66630
    gold_label_random = 2, sentence2_has_negation = 1: n = 1992
Validation Data...
    gold_label_random = 0, sentence2_has_negation = 0: n = 22814
    gold_label_random = 0, sentence2_has_negation = 1: n = 4634
    gold_label_random = 1, sentence2_has_negation = 0: n = 26949
    gold_label_random = 1, sentence2_has_negation = 1: n = 613
    gold_label_random = 2, sentence2_has_negation = 0: n = 26655
    gold_label_random = 2, sentence2_has_negation = 1: n = 797
Test Data...
    gold_label_random = 0, sentence2_has_negation = 0: n = 34597
    gold_label_random = 0, sentence2_has_negation = 1: n = 6655
    gold_label_random = 1, sentence2_has_negation = 0: n = 40496
    gold_label_random = 1, sentence2_has_negation = 1: n = 886
    gold_label_random = 2, sentence2_has_negation = 0: n = 39930
    gold_label_random = 2, sentence2_has_negation = 1: n = 1148

Epoch [0]:
Training:
Traceback (most recent call last):
  File "run_expt.py", line 327, in <module>
    main(args)
  File "run_expt.py", line 203, in main
    wandb=wandb if args.wandb else None,
  File "/home/gaotang/jtt/train.py", line 307, in train
    wandb=wandb,
  File "/home/gaotang/jtt/train.py", line 90, in run_epoch
    labels=y,
  File "/home/gaotang/jtt/venv/lib/python3.6/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/gaotang/jtt/venv/lib/python3.6/site-packages/pytorch_transformers/modeling_bert.py", line 963, in forward
    attention_mask=attention_mask, head_mask=head_mask)
  File "/home/gaotang/jtt/venv/lib/python3.6/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/gaotang/jtt/venv/lib/python3.6/site-packages/pytorch_transformers/modeling_bert.py", line 707, in forward
    embedding_output = self.embeddings(input_ids, position_ids=position_ids, token_type_ids=token_type_ids)
  File "/home/gaotang/jtt/venv/lib/python3.6/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/gaotang/jtt/venv/lib/python3.6/site-packages/pytorch_transformers/modeling_bert.py", line 251, in forward
    words_embeddings = self.word_embeddings(input_ids)
  File "/home/gaotang/jtt/venv/lib/python3.6/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/gaotang/jtt/venv/lib/python3.6/site-packages/torch/nn/modules/sparse.py", line 158, in forward
    self.norm_type, self.scale_grad_by_freq, self.sparse)
  File "/home/gaotang/jtt/venv/lib/python3.6/site-packages/torch/nn/functional.py", line 1916, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Input, output and indices must be on the current device
wandb: Waiting for W&B process to finish, PID 96865
wandb: Program failed with code 1.  Press ctrl-c to abort syncing.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: wandb/run-20220630_211740-1fcbil74/logs/debug.log
wandb: Find internal logs for this run at: wandb/run-20220630_211740-1fcbil74/logs/debug-internal.log
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced eager-planet-4: https://wandb.ai/gaotang/spurious_MultiNLI/runs/1fcbil74

Done
Traceback (most recent call last):
  File "process_training.py", line 140, in <module>
    main(args)
  File "process_training.py", line 31, in main
    train_df = pd.read_csv(os.path.join(data_dir, f"output_train_epoch_{final_epoch}.csv"))
  File "/home/gaotang/jtt/venv/lib/python3.6/site-packages/pandas/io/parsers.py", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "/home/gaotang/jtt/venv/lib/python3.6/site-packages/pandas/io/parsers.py", line 452, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "/home/gaotang/jtt/venv/lib/python3.6/site-packages/pandas/io/parsers.py", line 936, in __init__
    self._make_engine(self.engine)
  File "/home/gaotang/jtt/venv/lib/python3.6/site-packages/pandas/io/parsers.py", line 1168, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "/home/gaotang/jtt/venv/lib/python3.6/site-packages/pandas/io/parsers.py", line 1998, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas/_libs/parsers.pyx", line 382, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas/_libs/parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] No such file or directory: 'results/MultiNLI/MultiNLI_sample_exp/ERM_upweight_0_epochs_5_lr_2e-05_weight_decay_0.0_nobert/model_outputs/output_train_epoch_2.csv'
bash: results/MultiNLI/MultiNLI_sample_exp/train_downstream_ERM_upweight_0_epochs_5_lr_2e-05_weight_decay_0.0/final_epoch2/JTT_upweight_4_epochs_5_lr_2e-05_weight_decay_0/job.sh: No such file or directory
Traceback (most recent call last):
  File "analysis.py", line 227, in <module>
    folder for folder in os.listdir(args.training_output_dir)
FileNotFoundError: [Errno 2] No such file or directory: 'results/MultiNLI/MultiNLI_sample_exp/train_downstream_ERM_upweight_0_epochs_5_lr_2e-05_weight_decay_0.0/final_epoch2/'
