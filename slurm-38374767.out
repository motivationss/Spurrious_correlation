python run_expt.py -s confounder -d jigsaw -t toxicity -c identity_any --batch_size 24 --root_dir ./jigsaw --n_epochs 3 --aug_col None --log_dir results/jigsaw/jigsaw_sample_exp/ERM_upweight_0_epochs_3_lr_2e-05_weight_decay_0.0/model_outputs --metadata_path results/jigsaw/jigsaw_sample_exp/metadata_aug.csv --lr 2e-05 --weight_decay 0.0 --up_weight 0 --metadata_csv_name all_data_with_identities.csv  --model bert-base-uncased --use_bert_params 0 --wandb --loss_type erm

saved in results/jigsaw/jigsaw_sample_exp/ERM_upweight_0_epochs_3_lr_2e-05_weight_decay_0.0/job.sh


wandb: Currently logged in as: gaotang. Use `wandb login --relogin` to force relogin





 WARNING, Using bert-base-uncased without using BERT HYPER-PARAMS 







WARNING: You are using 'all_data_with_identities.csv' instead of the default 'metadata.csv'.


wandb: Tracking run with wandb version 0.12.18
wandb: Run data is saved locally in /home/gaotang/jtt/wandb/run-20220620_214309-3uey0nsf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rare-brook-16
wandb: ‚≠êÔ∏è View project at https://wandb.ai/gaotang/spurious_jigsaw
wandb: üöÄ View run at https://wandb.ai/gaotang/spurious_jigsaw/runs/3uey0nsf
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Dataset: jigsaw
Shift type: confounder
Wandb: True
Project name: spurious
Target name: toxicity
Confounder names: ['identity_any']
Up weight: 0
Resume: False
Minority fraction: None
Imbalance ratio: None
Fraction: 1.0
Root dir: ./jigsaw
Reweight groups: False
Augment data: False
Val fraction: 0.1
Loss type: erm
Alpha: 0.2
Generalization adjustment: 0.0
Automatic adjustment: False
Robust step size: 0.01
Joint dro alpha: 1
Use normalized loss: False
Btl: False
Hinge: False
Model: bert-base-uncased
Train from scratch: False
N epochs: 3
Batch size: 24
Lr: 2e-05
Scheduler: False
Weight decay: 0.0
Gamma: 0.1
Minimum variational weight: 0
Seed: 0
Show progress: False
Log dir: results/jigsaw/jigsaw_sample_exp/ERM_upweight_0_epochs_3_lr_2e-05_weight_decay_0.0/model_outputs
Log every: 50
Save step: 10
Save best: False
Save last: False
Use bert params: 0
Num folds per sweep: 5
Num sweeps: 4
Q: 0.7
Metadata csv name: all_data_with_identities.csv
Fold: None
Metadata path: results/jigsaw/jigsaw_sample_exp/metadata_aug.csv
Aug col: None

metadata_csv_name: all_data_with_identities.csv


WARNING: aug_col is not being used.


Training Data...
    toxicity = 0, identity_any = 0: n = 143628
    toxicity = 0, identity_any = 1: n = 94895
    toxicity = 1, identity_any = 0: n = 11940
    toxicity = 1, identity_any = 1: n = 18575
Validation Data...
    toxicity = 0, identity_any = 0: n = 24366
    toxicity = 0, identity_any = 1: n = 15759
    toxicity = 1, identity_any = 0: n = 1967
    toxicity = 1, identity_any = 1: n = 3088
Test Data...
    toxicity = 0, identity_any = 0: n = 72373
    toxicity = 0, identity_any = 1: n = 46185
    toxicity = 1, identity_any = 0: n = 6063
    toxicity = 1, identity_any = 1: n = 9161
n_classes = 2
/home/gaotang/jtt/venv/lib/python3.6/site-packages/torch/cuda/__init__.py:104: UserWarning: 
NVIDIA A40 with CUDA capability sm_86 is not compatible with the current PyTorch installation.
The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70 sm_75.
If you want to use the NVIDIA A40 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/

  warnings.warn(incompatible_device_warn.format(device_name, capability, " ".join(arch_list), device_name))
Traceback (most recent call last):
  File "run_expt.py", line 321, in <module>
    main(args)
  File "run_expt.py", line 197, in main
    wandb=wandb if args.wandb else None,
  File "/home/gaotang/jtt/train.py", line 201, in train
    joint_dro_alpha=args.joint_dro_alpha,
  File "/home/gaotang/jtt/loss.py", line 38, in __init__
    self.group_frac = self.group_counts / self.group_counts.sum()
RuntimeError: CUDA error: no kernel image is available for execution on the device
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: \ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: \ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: Synced rare-brook-16: https://wandb.ai/gaotang/spurious_jigsaw/runs/3uey0nsf
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220620_214309-3uey0nsf/logs
Done
