wandb: Currently logged in as: gaotang (use `wandb login --relogin` to force relogin)





 Using bert params 





wandb: wandb version 0.12.20 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.10.8
wandb: Syncing run stellar-moon-11
wandb: ‚≠êÔ∏è View project at https://wandb.ai/gaotang/spurious_MultiNLI
wandb: üöÄ View run at https://wandb.ai/gaotang/spurious_MultiNLI/runs/3g1u8x95
wandb: Run data is saved locally in wandb/run-20220703_174144-3g1u8x95
wandb: Run `wandb off` to turn off syncing.
Dataset: MultiNLI
Shift type: confounder
Wandb: True
Project name: spurious
Target name: gold_label_random
Confounder names: ['sentence2_has_negation']
Up weight: 100
Resume: False
Minority fraction: None
Imbalance ratio: None
Fraction: 1.0
Root dir: ./
Reweight groups: False
Augment data: False
Val fraction: 0.1
Loss type: erm
Alpha: 0.2
Generalization adjustment: 0.0
Automatic adjustment: False
Robust step size: 0.01
Joint dro alpha: 1
Use normalized loss: False
Btl: False
Hinge: False
Model: bert
Train from scratch: False
N epochs: 5
Batch size: 32
Lr: 1e-05
Scheduler: False
Weight decay: 0.1
Gamma: 0.1
Minimum variational weight: 0
Seed: 0
Show progress: False
Log dir: results/MultiNLI/MultiNLI_sample_exp/train_downstream_ERM_upweight_0_epochs_5_lr_2e-05_weight_decay_0.0/final_epoch2/JTT_upweight_100_epochs_5_lr_1e-05_weight_decay_0.1/model_outputs
Log every: 50
Save step: 10
Save best: False
Save last: False
Use bert params: 1
Num folds per sweep: 5
Num sweeps: 4
Q: 0.7
Metadata csv name: metadata.csv
Fold: None
Metadata path: results/MultiNLI/MultiNLI_sample_exp/train_downstream_ERM_upweight_0_epochs_5_lr_2e-05_weight_decay_0.0/final_epoch2/metadata_aug.csv
Aug col: wrong_1_times
Max grad norm: 1.0
Adam epsilon: 1e-08
Warmup steps: 0

length of train_data:  206175
length of test_data:  123712
length of val_data:  82462
args fold:  None
len 206175 61219
Up-weight factor: 100
Training Data...
    gold_label_random = 0, sentence2_has_negation = 0: n = 2379498
    gold_label_random = 0, sentence2_has_negation = 1: n = 69958
    gold_label_random = 1, sentence2_has_negation = 0: n = 1523676
    gold_label_random = 1, sentence2_has_negation = 1: n = 51821
    gold_label_random = 2, sentence2_has_negation = 0: n = 2167730
    gold_label_random = 2, sentence2_has_negation = 1: n = 135392
Validation Data...
    gold_label_random = 0, sentence2_has_negation = 0: n = 22814
    gold_label_random = 0, sentence2_has_negation = 1: n = 4634
    gold_label_random = 1, sentence2_has_negation = 0: n = 26949
    gold_label_random = 1, sentence2_has_negation = 1: n = 613
    gold_label_random = 2, sentence2_has_negation = 0: n = 26655
    gold_label_random = 2, sentence2_has_negation = 1: n = 797
Test Data...
    gold_label_random = 0, sentence2_has_negation = 0: n = 34597
    gold_label_random = 0, sentence2_has_negation = 1: n = 6655
    gold_label_random = 1, sentence2_has_negation = 0: n = 40496
    gold_label_random = 1, sentence2_has_negation = 1: n = 886
    gold_label_random = 2, sentence2_has_negation = 0: n = 39930
    gold_label_random = 2, sentence2_has_negation = 1: n = 1148

t_total is 988765


Epoch [0]:
Training:
/home/gaotang/jtt/venv/lib/python3.6/site-packages/torch/nn/modules/module.py:785: UserWarning: Using a non-full backward hook when outputs are generated by different autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_output. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when outputs are generated by different autograd Nodes "
/home/gaotang/jtt/venv/lib/python3.6/site-packages/torch/nn/modules/module.py:795: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Traceback (most recent call last):
  File "run_expt.py", line 327, in <module>
    main(args)
  File "run_expt.py", line 203, in main
    wandb=wandb if args.wandb else None,
  File "/home/gaotang/jtt/train.py", line 313, in train
    wandb=wandb,
  File "/home/gaotang/jtt/train.py", line 99, in run_epoch
    outputs[idx] += model.b[i] * lamd
IndexError: index 31439 is out of bounds for dimension 0 with size 11788
wandb: Waiting for W&B process to finish, PID 260712
wandb: Program failed with code 1.  Press ctrl-c to abort syncing.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: wandb/run-20220703_174144-3g1u8x95/logs/debug.log
wandb: Find internal logs for this run at: wandb/run-20220703_174144-3g1u8x95/logs/debug-internal.log
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced stellar-moon-11: https://wandb.ai/gaotang/spurious_MultiNLI/runs/3g1u8x95

Done
