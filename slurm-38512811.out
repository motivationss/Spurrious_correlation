python run_expt.py -s confounder -d jigsaw -t toxicity -c identity_any --batch_size 24 --root_dir ./jigsaw --n_epochs 3 --aug_col None --log_dir results/jigsaw/jigsaw_sample_exp/ERM_upweight_0_epochs_3_lr_2e-05_weight_decay_0.0/model_outputs --metadata_path results/jigsaw/jigsaw_sample_exp/metadata_aug.csv --lr 2e-05 --weight_decay 0.0 --up_weight 0 --metadata_csv_name all_data_with_identities.csv  --model bert-base-uncased --use_bert_params 0 --loss_type erm

saved in results/jigsaw/jigsaw_sample_exp/ERM_upweight_0_epochs_3_lr_2e-05_weight_decay_0.0/job.sh


wandb: Currently logged in as: gaotang (use `wandb login --relogin` to force relogin)





 WARNING, Using bert-base-uncased without using BERT HYPER-PARAMS 







WARNING: You are using 'all_data_with_identities.csv' instead of the default 'metadata.csv'.


wandb: wandb version 0.12.19 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.10.8
wandb: Syncing run pious-haze-61
wandb: ‚≠êÔ∏è View project at https://wandb.ai/gaotang/spurious_jigsaw
wandb: üöÄ View run at https://wandb.ai/gaotang/spurious_jigsaw/runs/3klxts0v
wandb: Run data is saved locally in wandb/run-20220625_225347-3klxts0v
wandb: Run `wandb off` to turn off syncing.
Dataset: jigsaw
Shift type: confounder
Wandb: True
Project name: spurious
Target name: toxicity
Confounder names: ['identity_any']
Up weight: 0
Resume: False
Minority fraction: None
Imbalance ratio: None
Fraction: 1.0
Root dir: ./jigsaw
Reweight groups: False
Augment data: False
Val fraction: 0.1
Loss type: erm
Alpha: 0.2
Generalization adjustment: 0.0
Automatic adjustment: False
Robust step size: 0.01
Joint dro alpha: 1
Use normalized loss: False
Btl: False
Hinge: False
Model: bert-base-uncased
Train from scratch: False
N epochs: 3
Batch size: 24
Lr: 2e-05
Scheduler: False
Weight decay: 0.0
Gamma: 0.1
Minimum variational weight: 0
Seed: 0
Show progress: False
Log dir: results/jigsaw/jigsaw_sample_exp/ERM_upweight_0_epochs_3_lr_2e-05_weight_decay_0.0/model_outputs
Log every: 50
Save step: 10
Save best: False
Save last: False
Use bert params: 0
Num folds per sweep: 5
Num sweeps: 4
Q: 0.7
Metadata csv name: all_data_with_identities.csv
Fold: None
Metadata path: results/jigsaw/jigsaw_sample_exp/metadata_aug.csv
Aug col: None

metadata_csv_name: all_data_with_identities.csv
length of train_data:  269038
length of test_data:  133782
length of val_data:  45180
args fold:  None


WARNING: aug_col is not being used.


/home/gaotang/jtt/venv/lib/python3.6/site-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  cpuset_checked))
Training Data...
    toxicity = 0, identity_any = 0: n = 143628
    toxicity = 0, identity_any = 1: n = 94895
    toxicity = 1, identity_any = 0: n = 11940
    toxicity = 1, identity_any = 1: n = 18575
Validation Data...
    toxicity = 0, identity_any = 0: n = 24366
    toxicity = 0, identity_any = 1: n = 15759
    toxicity = 1, identity_any = 0: n = 1967
    toxicity = 1, identity_any = 1: n = 3088
Test Data...
    toxicity = 0, identity_any = 0: n = 72373
    toxicity = 0, identity_any = 1: n = 46185
    toxicity = 1, identity_any = 0: n = 6063
    toxicity = 1, identity_any = 1: n = 9161
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
n_classes = 2
/home/gaotang/jtt/train.py:268: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  b = torch.tensor(torch.ones(448000, 2), requires_grad = True)

Epoch [0]:
Training:
/home/gaotang/jtt/venv/lib/python3.6/site-packages/torch/nn/modules/module.py:760: UserWarning: Using non-full backward hooks on a Module that does not return a single Tensor or a tuple of Tensors is deprecated and will be removed in future versions. This hook will be missing some of the grad_output. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using non-full backward hooks on a Module that does not return a "
/home/gaotang/jtt/venv/lib/python3.6/site-packages/torch/nn/modules/module.py:795: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Average incurred loss: 0.609  
Average sample loss: 0.609  
Average acc: 0.656  
  toxicity = 0, identity_any = 0  [n = 636]:	loss = 0.561  exp loss = 0.359  adjusted loss = 0.359  adv prob = 0.250000   acc = 0.717
  toxicity = 0, identity_any = 1  [n = 419]:	loss = 0.571  exp loss = 0.352  adjusted loss = 0.352  adv prob = 0.250000   acc = 0.675
  toxicity = 1, identity_any = 0  [n = 56]:	loss = 0.946  exp loss = 1.291  adjusted loss = 1.291  adv prob = 0.250000   acc = 0.304
  toxicity = 1, identity_any = 1  [n = 89]:	loss = 0.917  exp loss = 1.208  adjusted loss = 1.208  adv prob = 0.250000   acc = 0.348
Average incurred loss: 0.411  
Average sample loss: 0.411  
Average acc: 0.867  
  toxicity = 0, identity_any = 0  [n = 656]:	loss = 0.211  exp loss = 0.184  adjusted loss = 0.184  adv prob = 0.250000   acc = 1.000
  toxicity = 0, identity_any = 1  [n = 384]:	loss = 0.205  exp loss = 0.185  adjusted loss = 0.185  adv prob = 0.250000   acc = 1.000
  toxicity = 1, identity_any = 0  [n = 63]:	loss = 1.725  exp loss = 1.806  adjusted loss = 1.806  adv prob = 0.250000   acc = 0.000
  toxicity = 1, identity_any = 1  [n = 97]:	loss = 1.723  exp loss = 1.843  adjusted loss = 1.843  adv prob = 0.250000   acc = 0.000
Average incurred loss: 0.348  
Average sample loss: 0.348  
Average acc: 0.895  
  toxicity = 0, identity_any = 0  [n = 649]:	loss = 0.154  exp loss = 0.141  adjusted loss = 0.141  adv prob = 0.250000   acc = 1.000
  toxicity = 0, identity_any = 1  [n = 425]:	loss = 0.146  exp loss = 0.135  adjusted loss = 0.135  adv prob = 0.250000   acc = 1.000
  toxicity = 1, identity_any = 0  [n = 60]:	loss = 1.989  exp loss = 2.062  adjusted loss = 2.062  adv prob = 0.250000   acc = 0.000
  toxicity = 1, identity_any = 1  [n = 66]:	loss = 2.065  exp loss = 2.216  adjusted loss = 2.216  adv prob = 0.250000   acc = 0.000
Average incurred loss: 0.341  
Average sample loss: 0.341  
Average acc: 0.894  
  toxicity = 0, identity_any = 0  [n = 625]:	loss = 0.125  exp loss = 0.123  adjusted loss = 0.123  adv prob = 0.250000   acc = 1.000
  toxicity = 0, identity_any = 1  [n = 448]:	loss = 0.121  exp loss = 0.117  adjusted loss = 0.117  adv prob = 0.250000   acc = 1.000
  toxicity = 1, identity_any = 0  [n = 55]:	loss = 2.199  exp loss = 2.186  adjusted loss = 2.186  adv prob = 0.250000   acc = 0.000
  toxicity = 1, identity_any = 1  [n = 72]:	loss = 2.158  exp loss = 2.158  adjusted loss = 2.158  adv prob = 0.250000   acc = 0.000
Average incurred loss: 0.329  
Average sample loss: 0.329  
Average acc: 0.902  
  toxicity = 0, identity_any = 0  [n = 639]:	loss = 0.120  exp loss = 0.120  adjusted loss = 0.120  adv prob = 0.250000   acc = 1.000
  toxicity = 0, identity_any = 1  [n = 443]:	loss = 0.114  exp loss = 0.111  adjusted loss = 0.111  adv prob = 0.250000   acc = 1.000
  toxicity = 1, identity_any = 0  [n = 50]:	loss = 2.233  exp loss = 2.213  adjusted loss = 2.213  adv prob = 0.250000   acc = 0.000
  toxicity = 1, identity_any = 1  [n = 68]:	loss = 2.289  exp loss = 2.284  adjusted loss = 2.284  adv prob = 0.250000   acc = 0.000
Average incurred loss: 0.367  
Average sample loss: 0.367  
Average acc: 0.883  
  toxicity = 0, identity_any = 0  [n = 625]:	loss = 0.111  exp loss = 0.115  adjusted loss = 0.115  adv prob = 0.250000   acc = 1.000
  toxicity = 0, identity_any = 1  [n = 435]:	loss = 0.106  exp loss = 0.110  adjusted loss = 0.110  adv prob = 0.250000   acc = 1.000
  toxicity = 1, identity_any = 0  [n = 55]:	loss = 2.277  exp loss = 2.200  adjusted loss = 2.200  adv prob = 0.250000   acc = 0.000
  toxicity = 1, identity_any = 1  [n = 85]:	loss = 2.349  exp loss = 2.264  adjusted loss = 2.264  adv prob = 0.250000   acc = 0.000
Average incurred loss: 0.336  
Average sample loss: 0.336  
Average acc: 0.897  
  toxicity = 0, identity_any = 0  [n = 605]:	loss = 0.124  exp loss = 0.120  adjusted loss = 0.120  adv prob = 0.250000   acc = 1.000
  toxicity = 0, identity_any = 1  [n = 472]:	loss = 0.121  exp loss = 0.123  adjusted loss = 0.123  adv prob = 0.250000   acc = 0.998
  toxicity = 1, identity_any = 0  [n = 56]:	loss = 2.167  exp loss = 2.120  adjusted loss = 2.120  adv prob = 0.250000   acc = 0.000
  toxicity = 1, identity_any = 1  [n = 67]:	loss = 2.240  exp loss = 2.268  adjusted loss = 2.268  adv prob = 0.250000   acc = 0.000
Average incurred loss: 0.375  
Average sample loss: 0.375  
Average acc: 0.879  
  toxicity = 0, identity_any = 0  [n = 633]:	loss = 0.123  exp loss = 0.124  adjusted loss = 0.124  adv prob = 0.250000   acc = 1.000
  toxicity = 0, identity_any = 1  [n = 422]:	loss = 0.117  exp loss = 0.117  adjusted loss = 0.117  adv prob = 0.250000   acc = 1.000
  toxicity = 1, identity_any = 0  [n = 62]:	loss = 2.166  exp loss = 2.202  adjusted loss = 2.202  adv prob = 0.250000   acc = 0.000
  toxicity = 1, identity_any = 1  [n = 83]:	loss = 2.268  exp loss = 2.290  adjusted loss = 2.290  adv prob = 0.250000   acc = 0.000
Average incurred loss: 0.388  
Average sample loss: 0.388  
Average acc: 0.873  
  toxicity = 0, identity_any = 0  [n = 640]:	loss = 0.129  exp loss = 0.140  adjusted loss = 0.140  adv prob = 0.250000   acc = 1.000
  toxicity = 0, identity_any = 1  [n = 408]:	loss = 0.119  exp loss = 0.122  adjusted loss = 0.122  adv prob = 0.250000   acc = 1.000
  toxicity = 1, identity_any = 0  [n = 54]:	loss = 2.192  exp loss = 2.168  adjusted loss = 2.168  adv prob = 0.250000   acc = 0.000
  toxicity = 1, identity_any = 1  [n = 98]:	loss = 2.205  exp loss = 2.199  adjusted loss = 2.199  adv prob = 0.250000   acc = 0.000
Average incurred loss: 0.336  
Average sample loss: 0.336  
Average acc: 0.896  
  toxicity = 0, identity_any = 0  [n = 628]:	loss = 0.134  exp loss = 0.132  adjusted loss = 0.132  adv prob = 0.250000   acc = 1.000
  toxicity = 0, identity_any = 1  [n = 448]:	loss = 0.132  exp loss = 0.127  adjusted loss = 0.127  adv prob = 0.250000   acc = 0.998
  toxicity = 1, identity_any = 0  [n = 46]:	loss = 2.086  exp loss = 2.093  adjusted loss = 2.093  adv prob = 0.250000   acc = 0.000
  toxicity = 1, identity_any = 1  [n = 78]:	loss = 2.106  exp loss = 2.066  adjusted loss = 2.066  adv prob = 0.250000   acc = 0.000
Average incurred loss: 0.362  
Average sample loss: 0.362  
Average acc: 0.883  
  toxicity = 0, identity_any = 0  [n = 641]:	loss = 0.123  exp loss = 0.125  adjusted loss = 0.125  adv prob = 0.250000   acc = 1.000
  toxicity = 0, identity_any = 1  [n = 418]:	loss = 0.119  exp loss = 0.120  adjusted loss = 0.120  adv prob = 0.250000   acc = 1.000
  toxicity = 1, identity_any = 0  [n = 52]:	loss = 2.111  exp loss = 2.179  adjusted loss = 2.179  adv prob = 0.250000   acc = 0.000
  toxicity = 1, identity_any = 1  [n = 89]:	loss = 2.196  exp loss = 2.197  adjusted loss = 2.197  adv prob = 0.250000   acc = 0.000
Average incurred loss: 0.325  
Average sample loss: 0.325  
Average acc: 0.900  
  toxicity = 0, identity_any = 0  [n = 655]:	loss = 0.121  exp loss = 0.118  adjusted loss = 0.118  adv prob = 0.250000   acc = 1.000
  toxicity = 0, identity_any = 1  [n = 425]:	loss = 0.115  exp loss = 0.110  adjusted loss = 0.110  adv prob = 0.250000   acc = 1.000
  toxicity = 1, identity_any = 0  [n = 50]:	loss = 2.195  exp loss = 2.239  adjusted loss = 2.239  adv prob = 0.250000   acc = 0.000
  toxicity = 1, identity_any = 1  [n = 70]:	loss = 2.182  exp loss = 2.171  adjusted loss = 2.171  adv prob = 0.250000   acc = 0.000
Average incurred loss: 0.336  
Average sample loss: 0.336  
Average acc: 0.897  
  toxicity = 0, identity_any = 0  [n = 634]:	loss = 0.116  exp loss = 0.114  adjusted loss = 0.114  adv prob = 0.250000   acc = 1.000
  toxicity = 0, identity_any = 1  [n = 442]:	loss = 0.108  exp loss = 0.107  adjusted loss = 0.107  adv prob = 0.250000   acc = 1.000
  toxicity = 1, identity_any = 0  [n = 47]:	loss = 2.219  exp loss = 2.224  adjusted loss = 2.224  adv prob = 0.250000   acc = 0.000
  toxicity = 1, identity_any = 1  [n = 77]:	loss = 2.316  exp loss = 2.264  adjusted loss = 2.264  adv prob = 0.250000   acc = 0.000
Average incurred loss: 0.386  
Average sample loss: 0.386  
Average acc: 0.875  
  toxicity = 0, identity_any = 0  [n = 626]:	loss = 0.123  exp loss = 0.127  adjusted loss = 0.127  adv prob = 0.250000   acc = 1.000
  toxicity = 0, identity_any = 1  [n = 424]:	loss = 0.116  exp loss = 0.121  adjusted loss = 0.121  adv prob = 0.250000   acc = 1.000
  toxicity = 1, identity_any = 0  [n = 43]:	loss = 2.169  exp loss = 2.181  adjusted loss = 2.181  adv prob = 0.250000   acc = 0.000
  toxicity = 1, identity_any = 1  [n = 107]:	loss = 2.275  exp loss = 2.190  adjusted loss = 2.190  adv prob = 0.250000   acc = 0.000
Average incurred loss: 0.354  
Average sample loss: 0.354  
Average acc: 0.888  
  toxicity = 0, identity_any = 0  [n = 667]:	loss = 0.129  exp loss = 0.125  adjusted loss = 0.125  adv prob = 0.250000   acc = 1.000
  toxicity = 0, identity_any = 1  [n = 399]:	loss = 0.121  exp loss = 0.121  adjusted loss = 0.121  adv prob = 0.250000   acc = 1.000
  toxicity = 1, identity_any = 0  [n = 61]:	loss = 2.077  exp loss = 2.085  adjusted loss = 2.085  adv prob = 0.250000   acc = 0.000
  toxicity = 1, identity_any = 1  [n = 73]:	loss = 2.241  exp loss = 2.235  adjusted loss = 2.235  adv prob = 0.250000   acc = 0.000
Average incurred loss: 0.379  
Average sample loss: 0.379  
Average acc: 0.877  
  toxicity = 0, identity_any = 0  [n = 626]:	loss = 0.128  exp loss = 0.131  adjusted loss = 0.131  adv prob = 0.250000   acc = 1.000
  toxicity = 0, identity_any = 1  [n = 427]:	loss = 0.121  exp loss = 0.119  adjusted loss = 0.119  adv prob = 0.250000   acc = 1.000
  toxicity = 1, identity_any = 0  [n = 63]:	loss = 2.161  exp loss = 2.135  adjusted loss = 2.135  adv prob = 0.250000   acc = 0.000
  toxicity = 1, identity_any = 1  [n = 84]:	loss = 2.232  exp loss = 2.245  adjusted loss = 2.245  adv prob = 0.250000   acc = 0.000
Average incurred loss: 0.370  
Average sample loss: 0.370  
Average acc: 0.881  
  toxicity = 0, identity_any = 0  [n = 630]:	loss = 0.132  exp loss = 0.131  adjusted loss = 0.131  adv prob = 0.250000   acc = 1.000
  toxicity = 0, identity_any = 1  [n = 427]:	loss = 0.128  exp loss = 0.134  adjusted loss = 0.134  adv prob = 0.250000   acc = 1.000
  toxicity = 1, identity_any = 0  [n = 61]:	loss = 2.121  exp loss = 2.139  adjusted loss = 2.139  adv prob = 0.250000   acc = 0.000
  toxicity = 1, identity_any = 1  [n = 82]:	loss = 2.157  exp loss = 2.147  adjusted loss = 2.147  adv prob = 0.250000   acc = 0.000
Average incurred loss: 0.383  
Average sample loss: 0.383  
Average acc: 0.876  
  toxicity = 0, identity_any = 0  [n = 628]:	loss = 0.132  exp loss = 0.135  adjusted loss = 0.135  adv prob = 0.250000   acc = 1.000
  toxicity = 0, identity_any = 1  [n = 423]:	loss = 0.125  exp loss = 0.128  adjusted loss = 0.128  adv prob = 0.250000   acc = 1.000
  toxicity = 1, identity_any = 0  [n = 60]:	loss = 2.150  exp loss = 2.070  adjusted loss = 2.070  adv prob = 0.250000   acc = 0.000
  toxicity = 1, identity_any = 1  [n = 89]:	loss = 2.187  exp loss = 2.145  adjusted loss = 2.145  adv prob = 0.250000   acc = 0.000
Average incurred loss: 0.350  
Average sample loss: 0.350  
Average acc: 0.889  
  toxicity = 0, identity_any = 0  [n = 643]:	loss = 0.136  exp loss = 0.134  adjusted loss = 0.134  adv prob = 0.250000   acc = 1.000
  toxicity = 0, identity_any = 1  [n = 424]:	loss = 0.131  exp loss = 0.129  adjusted loss = 0.129  adv prob = 0.250000   acc = 1.000
  toxicity = 1, identity_any = 0  [n = 48]:	loss = 2.055  exp loss = 2.064  adjusted loss = 2.064  adv prob = 0.250000   acc = 0.000
  toxicity = 1, identity_any = 1  [n = 85]:	loss = 2.105  exp loss = 2.131  adjusted loss = 2.131  adv prob = 0.250000   acc = 0.000
Average incurred loss: 0.338  
Average sample loss: 0.338  
Average acc: 0.893  
  toxicity = 0, identity_any = 0  [n = 642]:	loss = 0.125  exp loss = 0.121  adjusted loss = 0.121  adv prob = 0.250000   acc = 1.000
  toxicity = 0, identity_any = 1  [n = 430]:	loss = 0.118  exp loss = 0.116  adjusted loss = 0.116  adv prob = 0.250000   acc = 1.000
  toxicity = 1, identity_any = 0  [n = 43]:	loss = 2.114  exp loss = 2.092  adjusted loss = 2.092  adv prob = 0.250000   acc = 0.000
  toxicity = 1, identity_any = 1  [n = 85]:	loss = 2.158  exp loss = 2.213  adjusted loss = 2.213  adv prob = 0.250000   acc = 0.000
Average incurred loss: 0.356  
Average sample loss: 0.356  
Average acc: 0.887  
  toxicity = 0, identity_any = 0  [n = 629]:	loss = 0.118  exp loss = 0.118  adjusted loss = 0.118  adv prob = 0.250000   acc = 1.000
  toxicity = 0, identity_any = 1  [n = 436]:	loss = 0.113  exp loss = 0.111  adjusted loss = 0.111  adv prob = 0.250000   acc = 1.000
  toxicity = 1, identity_any = 0  [n = 53]:	loss = 2.205  exp loss = 2.199  adjusted loss = 2.199  adv prob = 0.250000   acc = 0.000
  toxicity = 1, identity_any = 1  [n = 82]:	loss = 2.280  exp loss = 2.311  adjusted loss = 2.311  adv prob = 0.250000   acc = 0.000
Average incurred loss: 0.371  
Average sample loss: 0.371  
Average acc: 0.879  
  toxicity = 0, identity_any = 0  [n = 644]:	loss = 0.121  exp loss = 0.122  adjusted loss = 0.122  adv prob = 0.250000   acc = 1.000
  toxicity = 0, identity_any = 1  [n = 411]:	loss = 0.116  exp loss = 0.118  adjusted loss = 0.118  adv prob = 0.250000   acc = 1.000
  toxicity = 1, identity_any = 0  [n = 56]:	loss = 2.172  exp loss = 2.162  adjusted loss = 2.162  adv prob = 0.250000   acc = 0.000
  toxicity = 1, identity_any = 1  [n = 89]:	loss = 2.216  exp loss = 2.204  adjusted loss = 2.204  adv prob = 0.250000   acc = 0.000
Average incurred loss: 0.347  
Average sample loss: 0.347  
Average acc: 0.893  
  toxicity = 0, identity_any = 0  [n = 658]:	loss = 0.126  exp loss = 0.127  adjusted loss = 0.127  adv prob = 0.250000   acc = 1.000
  toxicity = 0, identity_any = 1  [n = 413]:	loss = 0.121  exp loss = 0.119  adjusted loss = 0.119  adv prob = 0.250000   acc = 1.000
  toxicity = 1, identity_any = 0  [n = 43]:	loss = 2.187  exp loss = 2.163  adjusted loss = 2.163  adv prob = 0.250000   acc = 0.000
  toxicity = 1, identity_any = 1  [n = 86]:	loss = 2.197  exp loss = 2.225  adjusted loss = 2.225  adv prob = 0.250000   acc = 0.000
Average incurred loss: 0.330  
Average sample loss: 0.330  
Average acc: 0.896  
  toxicity = 0, identity_any = 0  [n = 651]:	loss = 0.120  exp loss = 0.116  adjusted loss = 0.116  adv prob = 0.250000   acc = 1.000
  toxicity = 0, identity_any = 1  [n = 425]:	loss = 0.118  exp loss = 0.114  adjusted loss = 0.114  adv prob = 0.250000   acc = 0.998
  toxicity = 1, identity_any = 0  [n = 54]:	loss = 2.128  exp loss = 2.186  adjusted loss = 2.186  adv prob = 0.250000   acc = 0.000
  toxicity = 1, identity_any = 1  [n = 70]:	loss = 2.172  exp loss = 2.176  adjusted loss = 2.176  adv prob = 0.250000   acc = 0.000
Average incurred loss: 0.372  
Average sample loss: 0.372  
Average acc: 0.878  
  toxicity = 0, identity_any = 0  [n = 642]:	loss = 0.120  exp loss = 0.120  adjusted loss = 0.120  adv prob = 0.250000   acc = 1.000
  toxicity = 0, identity_any = 1  [n = 411]:	loss = 0.112  exp loss = 0.116  adjusted loss = 0.116  adv prob = 0.250000   acc = 1.000
  toxicity = 1, identity_any = 0  [n = 59]:	loss = 2.139  exp loss = 2.141  adjusted loss = 2.141  adv prob = 0.250000   acc = 0.000
  toxicity = 1, identity_any = 1  [n = 88]:	loss = 2.235  exp loss = 2.243  adjusted loss = 2.243  adv prob = 0.250000   acc = 0.000
Average incurred loss: 0.363  
Average sample loss: 0.363  
Average acc: 0.883  
  toxicity = 0, identity_any = 0  [n = 617]:	loss = 0.126  exp loss = 0.127  adjusted loss = 0.127  adv prob = 0.250000   acc = 1.000
  toxicity = 0, identity_any = 1  [n = 443]:	loss = 0.118  exp loss = 0.119  adjusted loss = 0.119  adv prob = 0.250000   acc = 1.000
  toxicity = 1, identity_any = 0  [n = 52]:	loss = 2.124  exp loss = 2.174  adjusted loss = 2.174  adv prob = 0.250000   acc = 0.000
  toxicity = 1, identity_any = 1  [n = 88]:	loss = 2.211  exp loss = 2.262  adjusted loss = 2.262  adv prob = 0.250000   acc = 0.000
Average incurred loss: 0.379  
Average sample loss: 0.379  
Average acc: 0.875  
  toxicity = 0, identity_any = 0  [n = 612]:	loss = 0.131  exp loss = 0.133  adjusted loss = 0.133  adv prob = 0.250000   acc = 1.000
  toxicity = 0, identity_any = 1  [n = 438]:	loss = 0.125  exp loss = 0.128  adjusted loss = 0.128  adv prob = 0.250000   acc = 1.000
  toxicity = 1, identity_any = 0  [n = 64]:	loss = 2.080  exp loss = 2.088  adjusted loss = 2.088  adv prob = 0.250000   acc = 0.000
  toxicity = 1, identity_any = 1  [n = 86]:	loss = 2.164  exp loss = 2.152  adjusted loss = 2.152  adv prob = 0.250000   acc = 0.000
Average incurred loss: 0.348  
Average sample loss: 0.348  
Average acc: 0.892  
  toxicity = 0, identity_any = 0  [n = 605]:	loss = 0.130  exp loss = 0.127  adjusted loss = 0.127  adv prob = 0.250000   acc = 1.000
  toxicity = 0, identity_any = 1  [n = 465]:	loss = 0.125  exp loss = 0.127  adjusted loss = 0.127  adv prob = 0.250000   acc = 1.000
  toxicity = 1, identity_any = 0  [n = 52]:	loss = 2.114  exp loss = 2.089  adjusted loss = 2.089  adv prob = 0.250000   acc = 0.000
  toxicity = 1, identity_any = 1  [n = 78]:	loss = 2.188  exp loss = 2.147  adjusted loss = 2.147  adv prob = 0.250000   acc = 0.000
Average incurred loss: 0.384  
Average sample loss: 0.384  
Average acc: 0.870  
  toxicity = 0, identity_any = 0  [n = 657]:	loss = 0.130  exp loss = 0.132  adjusted loss = 0.132  adv prob = 0.250000   acc = 1.000
  toxicity = 0, identity_any = 1  [n = 387]:	loss = 0.123  exp loss = 0.127  adjusted loss = 0.127  adv prob = 0.250000   acc = 1.000
  toxicity = 1, identity_any = 0  [n = 62]:	loss = 2.081  exp loss = 2.088  adjusted loss = 2.088  adv prob = 0.250000   acc = 0.000
  toxicity = 1, identity_any = 1  [n = 94]:	loss = 2.109  exp loss = 2.039  adjusted loss = 2.039  adv prob = 0.250000   acc = 0.000
Average incurred loss: 0.358  
Average sample loss: 0.358  
Average acc: 0.884  
  toxicity = 0, identity_any = 0  [n = 659]:	loss = 0.136  exp loss = 0.136  adjusted loss = 0.136  adv prob = 0.250000   acc = 1.000
  toxicity = 0, identity_any = 1  [n = 402]:	loss = 0.129  exp loss = 0.131  adjusted loss = 0.131  adv prob = 0.250000   acc = 1.000
  toxicity = 1, identity_any = 0  [n = 56]:	loss = 2.069  exp loss = 2.078  adjusted loss = 2.078  adv prob = 0.250000   acc = 0.000
  toxicity = 1, identity_any = 1  [n = 83]:	loss = 2.073  exp loss = 2.090  adjusted loss = 2.090  adv prob = 0.250000   acc = 0.000
Average incurred loss: 0.365  
Average sample loss: 0.365  
Average acc: 0.881  
  toxicity = 0, identity_any = 0  [n = 627]:	loss = 0.131  exp loss = 0.132  adjusted loss = 0.132  adv prob = 0.250000   acc = 1.000
  toxicity = 0, identity_any = 1  [n = 430]:	loss = 0.120  exp loss = 0.121  adjusted loss = 0.121  adv prob = 0.250000   acc = 1.000
  toxicity = 1, identity_any = 0  [n = 54]:	loss = 2.079  exp loss = 2.003  adjusted loss = 2.003  adv prob = 0.250000   acc = 0.000
  toxicity = 1, identity_any = 1  [n = 89]:	loss = 2.156  exp loss = 2.183  adjusted loss = 2.183  adv prob = 0.250000   acc = 0.000
Average incurred loss: 0.346  
Average sample loss: 0.346  
Average acc: 0.890  
  toxicity = 0, identity_any = 0  [n = 651]:	loss = 0.124  exp loss = 0.123  adjusted loss = 0.123  adv prob = 0.250000   acc = 1.000
  toxicity = 0, identity_any = 1  [n = 417]:	loss = 0.118  exp loss = 0.118  adjusted loss = 0.118  adv prob = 0.250000   acc = 1.000
  toxicity = 1, identity_any = 0  [n = 51]:	loss = 2.107  exp loss = 2.100  adjusted loss = 2.100  adv prob = 0.250000   acc = 0.000
  toxicity = 1, identity_any = 1  [n = 81]:	loss = 2.198  exp loss = 2.247  adjusted loss = 2.247  adv prob = 0.250000   acc = 0.000
Average incurred loss: 0.354  
Average sample loss: 0.354  
Average acc: 0.885  
  toxicity = 0, identity_any = 0  [n = 657]:	loss = 0.126  exp loss = 0.127  adjusted loss = 0.127  adv prob = 0.250000   acc = 1.000
  toxicity = 0, identity_any = 1  [n = 405]:	loss = 0.119  exp loss = 0.117  adjusted loss = 0.117  adv prob = 0.250000   acc = 1.000
  toxicity = 1, identity_any = 0  [n = 60]:	loss = 2.079  exp loss = 2.069  adjusted loss = 2.069  adv prob = 0.250000   acc = 0.000
  toxicity = 1, identity_any = 1  [n = 78]:	loss = 2.165  exp loss = 2.186  adjusted loss = 2.186  adv prob = 0.250000   acc = 0.000
Average incurred loss: 0.323  
Average sample loss: 0.323  
Average acc: 0.903  
  toxicity = 0, identity_any = 0  [n = 668]:	loss = 0.118  exp loss = 0.114  adjusted loss = 0.114  adv prob = 0.250000   acc = 1.000
  toxicity = 0, identity_any = 1  [n = 415]:	loss = 0.113  exp loss = 0.108  adjusted loss = 0.108  adv prob = 0.250000   acc = 1.000
  toxicity = 1, identity_any = 0  [n = 54]:	loss = 2.226  exp loss = 2.205  adjusted loss = 2.205  adv prob = 0.250000   acc = 0.000
  toxicity = 1, identity_any = 1  [n = 63]:	loss = 2.254  exp loss = 2.293  adjusted loss = 2.293  adv prob = 0.250000   acc = 0.000
Average incurred loss: 0.364  
Average sample loss: 0.364  
Average acc: 0.880  
  toxicity = 0, identity_any = 0  [n = 616]:	loss = 0.118  exp loss = 0.119  adjusted loss = 0.119  adv prob = 0.250000   acc = 1.000
  toxicity = 0, identity_any = 1  [n = 440]:	loss = 0.112  exp loss = 0.114  adjusted loss = 0.114  adv prob = 0.250000   acc = 1.000
  toxicity = 1, identity_any = 0  [n = 59]:	loss = 2.155  exp loss = 2.128  adjusted loss = 2.128  adv prob = 0.250000   acc = 0.000
  toxicity = 1, identity_any = 1  [n = 85]:	loss = 2.204  exp loss = 2.216  adjusted loss = 2.216  adv prob = 0.250000   acc = 0.000
Average incurred loss: 0.380  
Average sample loss: 0.380  
Average acc: 0.872  
  toxicity = 0, identity_any = 0  [n = 621]:	loss = 0.130  exp loss = 0.131  adjusted loss = 0.131  adv prob = 0.250000   acc = 1.000
  toxicity = 0, identity_any = 1  [n = 425]:	loss = 0.122  exp loss = 0.126  adjusted loss = 0.126  adv prob = 0.250000   acc = 1.000
  toxicity = 1, identity_any = 0  [n = 63]:	loss = 2.007  exp loss = 1.960  adjusted loss = 1.960  adv prob = 0.250000   acc = 0.000
  toxicity = 1, identity_any = 1  [n = 91]:	loss = 2.174  exp loss = 2.150  adjusted loss = 2.150  adv prob = 0.250000   acc = 0.000
Average incurred loss: 0.336  
Average sample loss: 0.336  
Average acc: 0.893  
  toxicity = 0, identity_any = 0  [n = 646]:	loss = 0.131  exp loss = 0.128  adjusted loss = 0.128  adv prob = 0.250000   acc = 1.000
  toxicity = 0, identity_any = 1  [n = 426]:	loss = 0.124  exp loss = 0.120  adjusted loss = 0.120  adv prob = 0.250000   acc = 1.000
  toxicity = 1, identity_any = 0  [n = 59]:	loss = 2.071  exp loss = 2.113  adjusted loss = 2.113  adv prob = 0.250000   acc = 0.000
  toxicity = 1, identity_any = 1  [n = 69]:	loss = 2.090  exp loss = 2.081  adjusted loss = 2.081  adv prob = 0.250000   acc = 0.000
slurmstepd: error: *** JOB 38512811 ON gl1513 CANCELLED AT 2022-06-25T22:59:35 ***
